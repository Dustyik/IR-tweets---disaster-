{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6103, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_fwf(\"signal1m_tweets_qrels\", names=['topic','iteration','document','rel'])\r\n",
    "\r\n",
    "df[df.document==645936801232912384]\r\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy as tw\n",
    "'''\n",
    "if need to log in to the twitter account:\n",
    "username = eugene_tan@mymail.sutd.edu.sg\n",
    "password = P@ssword1\n",
    "'''\n",
    "\n",
    "consumer_key= 'LKvavXktNliE3LL52KNK48oWd'\n",
    "consumer_secret= 'PsSoEz5JRYdrozOe8uBkgphm3YlTipYREexIyJQi4VDVZv9htI'\n",
    "access_token= '1411277462630146057-ATYdgNSGb3EGBFim8w1sQ8kRGOR7QH'\n",
    "access_token_secret= 'S3N1R58XNvrucIOgsKIxZY4CSfngnrd6XHIlWCu0k4PaP'\n",
    "bearer_token= 'AAAAAAAAAAAAAAAAAAAAAOskRQEAAAAA5JKli8luYWK%2BzRTWfn1kLwjv58s%3DWNBJ8KrY0xEwZBqxqLzCjde2fGWM3wBMeq4uyvWoLX5BNfIHHp'\n",
    "\n",
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scrape Data\r\n",
    "\r\n",
    "# def get_tweet(id):\r\n",
    "#     output = ''\r\n",
    "#     try:\r\n",
    "#         output = api.get_status(id=id.document).text\r\n",
    "#     except:\r\n",
    "#         pass\r\n",
    "#     return output\r\n",
    "\r\n",
    "# df['text']=df.apply(lambda x:get_tweet(x), axis=1)\r\n",
    "\r\n",
    "# df.head()\r\n",
    "# df = df[df.text!='']\r\n",
    "# df.to_csv(\"dataset_scrapped.csv\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                  topic  iteration            document  rel  \\\n",
       "0  00ecb565-8dcb-443e-875c-71babdce2269          0  638637447023194112    0   \n",
       "1  00ecb565-8dcb-443e-875c-71babdce2269          0  638727826393444352    1   \n",
       "2  00ecb565-8dcb-443e-875c-71babdce2269          0  639043756038074369    1   \n",
       "3  00ecb565-8dcb-443e-875c-71babdce2269          0  639219133859631104    0   \n",
       "4  00ecb565-8dcb-443e-875c-71babdce2269          0  639325701297041408    1   \n",
       "\n",
       "                                                text  \n",
       "0  Video:  Shippers look to grab a piece of North...  \n",
       "1  Police say woman was driving drunk with kids i...  \n",
       "2  Hit-and-Run Driver Seriously Injures Man in Po...  \n",
       "3  #UPDATE: Man in critical condition after being...  \n",
       "4  A Barrie woman that was run over by a drunk dr...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>iteration</th>\n      <th>document</th>\n      <th>rel</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>638637447023194112</td>\n      <td>0</td>\n      <td>Video:  Shippers look to grab a piece of North...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>638727826393444352</td>\n      <td>1</td>\n      <td>Police say woman was driving drunk with kids i...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>639043756038074369</td>\n      <td>1</td>\n      <td>Hit-and-Run Driver Seriously Injures Man in Po...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>639219133859631104</td>\n      <td>0</td>\n      <td>#UPDATE: Man in critical condition after being...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>639325701297041408</td>\n      <td>1</td>\n      <td>A Barrie woman that was run over by a drunk dr...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "data = pd.read_csv('dataset_scrapped.csv', names=['topic','iteration','document','rel', 'text'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                   topic  iteration            document  rel  \\\n",
       "0   00ecb565-8dcb-443e-875c-71babdce2269          0  638637447023194112    0   \n",
       "1   00ecb565-8dcb-443e-875c-71babdce2269          0  638727826393444352    1   \n",
       "2   00ecb565-8dcb-443e-875c-71babdce2269          0  639043756038074369    1   \n",
       "3   00ecb565-8dcb-443e-875c-71babdce2269          0  639219133859631104    0   \n",
       "4   00ecb565-8dcb-443e-875c-71babdce2269          0  639325701297041408    1   \n",
       "5   00ecb565-8dcb-443e-875c-71babdce2269          0  639362932363653120    0   \n",
       "6   00ecb565-8dcb-443e-875c-71babdce2269          0  639485269385678849    1   \n",
       "7   00ecb565-8dcb-443e-875c-71babdce2269          0  639490286196932608    0   \n",
       "8   00ecb565-8dcb-443e-875c-71babdce2269          0  639535449673850880    0   \n",
       "10  00ecb565-8dcb-443e-875c-71babdce2269          0  639662060368846848    1   \n",
       "\n",
       "                                                 text  \n",
       "0   video shipper look to grab a piec of northwest...  \n",
       "1   polic say woman wa drive drunk with kid in the...  \n",
       "2   driver serious injur man in portola district a...  \n",
       "3   updat man in critic condit after be shot in th...  \n",
       "4   a barri woman that wa run over by a drunk driv...  \n",
       "5   thi photo prove that there are sight in the no...  \n",
       "6   a man wa hit by a car while tri to chase after...  \n",
       "7   christopherokey the rule wa that he swung and ...  \n",
       "8   my friend wa do cpr on him he took a coupl of ...  \n",
       "10  rt the women who got out of her car to help ma...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>topic</th>\n      <th>iteration</th>\n      <th>document</th>\n      <th>rel</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>638637447023194112</td>\n      <td>0</td>\n      <td>video shipper look to grab a piec of northwest...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>638727826393444352</td>\n      <td>1</td>\n      <td>polic say woman wa drive drunk with kid in the...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>639043756038074369</td>\n      <td>1</td>\n      <td>driver serious injur man in portola district a...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>639219133859631104</td>\n      <td>0</td>\n      <td>updat man in critic condit after be shot in th...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>639325701297041408</td>\n      <td>1</td>\n      <td>a barri woman that wa run over by a drunk driv...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>639362932363653120</td>\n      <td>0</td>\n      <td>thi photo prove that there are sight in the no...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>639485269385678849</td>\n      <td>1</td>\n      <td>a man wa hit by a car while tri to chase after...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>639490286196932608</td>\n      <td>0</td>\n      <td>christopherokey the rule wa that he swung and ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>639535449673850880</td>\n      <td>0</td>\n      <td>my friend wa do cpr on him he took a coupl of ...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>00ecb565-8dcb-443e-875c-71babdce2269</td>\n      <td>0</td>\n      <td>639662060368846848</td>\n      <td>1</td>\n      <td>rt the women who got out of her car to help ma...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "data = pd.read_csv('dataset_scrapped.csv', names=['topic','iteration','document','rel', 'text'])\n",
    "\n",
    "porter = PorterStemmer() ## stemming reduces the word to root word\n",
    "words = words.words()\n",
    "# vsm = {}\n",
    "\n",
    "# def stem_lower(word):\n",
    "#     return porter.stem(word.lower())\n",
    "\n",
    "# for i in list(set(zip(data['document'], data['text']))):\n",
    "#     # print(i[0], i[1])\n",
    "#     tokens = word_tokenize(i[1])\n",
    "#     # print(len(tokens))\n",
    "#     tokens = list(filter(lambda x: x.isalpha(), tokens))\n",
    "#     # tokens = list(filter(lambda x: x in words, tokens)) ## this removes too much semantics\n",
    "#     tokens = list(map(stem_lower, tokens))\n",
    "#     # print(len(tokens))\n",
    "#     vsm[str(i[0])] = tokens\n",
    "\n",
    "def tokenize_stem_lower(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = list(filter(lambda x: x.isalpha(), tokens))\n",
    "    tokens = [porter.stem(x.lower()) for x in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "data['text']=data.apply(lambda x: tokenize_stem_lower(x.text), axis=1)\n",
    "\n",
    "vsm['645687105964912640']\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-e53737dfa3f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# # for epoch in range(10):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m# # model.save('/tmp/my_model.doc2vec')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# q = TaggedDocument('name is e', 'tag4')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1034\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_training_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1523\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# should be set by `build_vocab`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1524\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import gensim\n",
    "\n",
    "doc1 = TaggedDocument(word_tokenize('My name is eugene'), 'tag1')\n",
    "doc2 = TaggedDocument(word_tokenize('My name is tan'), 'tag2')\n",
    "doc3 = TaggedDocument(word_tokenize('My name is alpha'), 'tag3')\n",
    "sentences = [doc1, doc2, doc3]\n",
    "\n",
    "model = Doc2Vec(alpha=0.025, min_alpha=0.025)\n",
    "model.build_vocab(sentences)\n",
    "# # for epoch in range(10):\n",
    "model.train(sentences, total_examples=model.corpus_count, epochs=10)\n",
    "# # model.save('/tmp/my_model.doc2vec')\n",
    "# q = TaggedDocument('name is e', 'tag4')\n",
    "# tokens = word_tokenize('name is e')\n",
    "# query = model.infer_vector(tokens)\n",
    "# # print(model.docvecs.most_similar(positive=[query], topn=10))\n",
    "#     # model.alpha -= 0.002  # decrease the learning rate\n",
    "#     # model.min_alpha = model.alpha\n",
    "# print(model.dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    aaron  abad  abandon  abarrica  abba  abbey  \\\n",
       "document                                                          \n",
       "638637447023194112      0     0        0         0     0      0   \n",
       "638727826393444352      0     0        0         0     0      0   \n",
       "639043756038074369      0     0        0         0     0      0   \n",
       "639219133859631104      0     0        0         0     0      0   \n",
       "639325701297041408      0     0        0         0     0      0   \n",
       "\n",
       "                    abbeylynnmangu  abbott  abcgrandstand  abdel  ...  \\\n",
       "document                                                          ...   \n",
       "638637447023194112               0       0              0      0  ...   \n",
       "638727826393444352               0       0              0      0  ...   \n",
       "639043756038074369               0       0              0      0  ...   \n",
       "639219133859631104               0       0              0      0  ...   \n",
       "639325701297041408               0       0              0      0  ...   \n",
       "\n",
       "                    zippydazipst  zone  zsofimajor  zuckerberg  zuma  zurich  \\\n",
       "document                                                                       \n",
       "638637447023194112             0     0           0           0     0       0   \n",
       "638727826393444352             0     0           0           0     0       0   \n",
       "639043756038074369             0     0           0           0     0       0   \n",
       "639219133859631104             0     0           0           0     0       0   \n",
       "639325701297041408             0     0           0           0     0       0   \n",
       "\n",
       "                    zvishavan  zweigen  öbb  박근혜  \n",
       "document                                          \n",
       "638637447023194112          0        0    0    0  \n",
       "638727826393444352          0        0    0    0  \n",
       "639043756038074369          0        0    0    0  \n",
       "639219133859631104          0        0    0    0  \n",
       "639325701297041408          0        0    0    0  \n",
       "\n",
       "[5 rows x 9133 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>aaron</th>\n      <th>abad</th>\n      <th>abandon</th>\n      <th>abarrica</th>\n      <th>abba</th>\n      <th>abbey</th>\n      <th>abbeylynnmangu</th>\n      <th>abbott</th>\n      <th>abcgrandstand</th>\n      <th>abdel</th>\n      <th>...</th>\n      <th>zippydazipst</th>\n      <th>zone</th>\n      <th>zsofimajor</th>\n      <th>zuckerberg</th>\n      <th>zuma</th>\n      <th>zurich</th>\n      <th>zvishavan</th>\n      <th>zweigen</th>\n      <th>öbb</th>\n      <th>박근혜</th>\n    </tr>\n    <tr>\n      <th>document</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>638637447023194112</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>638727826393444352</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>639043756038074369</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>639219133859631104</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>639325701297041408</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 9133 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "'''\n",
    "CBOW tries to predict a word on the basis of its neighbors, while Skip Gram tries to predict the neighbors of a word.\n",
    "\n",
    "In simpler words, CBOW tends to find the probability of a word occurring in a context. So, it generalizes over all the different contexts in which a word can be used. Whereas SkipGram tends to study different contexts separately. Skip Gram needs more data to be trained contains more knowledge about the context.\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# model = Word2Vec(list(data.text), min_count=1,size= 50,workers=3, window =3, sg = 1)\n",
    "vec = CountVectorizer()\n",
    "term_doc = vec.fit_transform(list(data.text))\n",
    "td_df = pd.DataFrame(term_doc.toarray(), columns=vec.get_feature_names(), index=data.document)\n",
    "\n",
    "td_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a number, not 'csr_matrix'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-b6305a5816b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"hello my name is eugene\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtd_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;31m# to avoid recursive import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[0;32m    149\u001b[0m         Y = check_array(Y, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[0;32m    150\u001b[0m                         \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_all_finite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m                         estimator=estimator)\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprecomputed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    671\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query = pd.DataFrame(vec.fit_transform([\"hello my name is eugene\"]))\n",
    "\n",
    "cosine_similarity(td_df, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3334a78060f906efb30753b21a137354a590a32b6998c12cfe16e21065aab06"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}